{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddec1a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 81\n",
      "Saved features_intermediate.csv\n",
      "Saved TF-IDF vectorizer and SVD\n",
      "Label counts before relaxation:\n",
      " quality_label\n",
      "Low       40\n",
      "Medium    33\n",
      "High       8\n",
      "Name: count, dtype: int64\n",
      "Classes mapping: {'High': 0, 'Low': 1, 'Medium': 2}\n",
      "Class distribution: Counter({1: 40, 2: 33, 0: 8})\n",
      "Test Accuracy: 0.88\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.00      0.00      0.00         3\n",
      "         Low       1.00      1.00      1.00        12\n",
      "      Medium       0.77      1.00      0.87        10\n",
      "\n",
      "    accuracy                           0.88        25\n",
      "   macro avg       0.59      0.67      0.62        25\n",
      "weighted avg       0.79      0.88      0.83        25\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\miniconda3\\envs\\seo_app\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\Hp\\miniconda3\\envs\\seo_app\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\Hp\\miniconda3\\envs\\seo_app\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy (5-fold): 0.8022058823529411 [0.82352941 0.6875     0.9375     0.75       0.8125    ]\n",
      "Saved classifier and encoders to models/\n",
      "Top features: [('flesch', 0.10847617781788874), ('word_count', 0.06577506902266071), ('meta_count', 0.05200038688169177), ('svd_36', 0.04349649476763824), ('svd_14', 0.03837238983226825), ('svd_39', 0.03401888397953561), ('svd_22', 0.027561534936863426), ('svd_3', 0.026898548779812872), ('svd_6', 0.02545272675681532), ('svd_19', 0.025241207555555863), ('svd_5', 0.023695122732777362), ('svd_34', 0.023648850232819145), ('link_count', 0.021881050640847503), ('svd_47', 0.021399094420429258), ('svd_49', 0.019884796552567897)]\n"
     ]
    }
   ],
   "source": [
    "# train_seo_model.py\n",
    "import os, re, joblib\n",
    "import numpy as np, pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import textstat\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dded67b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- LOAD CSV (adjust path if necessary) ----------\n",
    "INPUT_CSV = r\"C:\\Users\\Hp\\Downloads\\data.csv\" # or \"data/data.csv\"\n",
    "df_raw = pd.read_csv(INPUT_CSV)\n",
    "print(\"Rows:\", len(df_raw))\n",
    "\n",
    "# --------- CLEAN HTML -> TEXT & BASIC META ----------\n",
    "def clean_html(html):\n",
    "    if pd.isna(html): \n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(str(html), \"html.parser\")\n",
    "    for tag in soup([\"script\",\"style\",\"noscript\",\"header\",\"footer\",\"nav\",\"form\"]):\n",
    "        tag.decompose()\n",
    "    text = soup.get_text(separator=\" \", strip=True)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "df_raw['clean_text'] = df_raw['html_content'].apply(clean_html)\n",
    "df_raw['word_count'] = df_raw['clean_text'].apply(lambda t: len(t.split()))\n",
    "def tag_counts(html):\n",
    "    if pd.isna(html): return 0,0\n",
    "    s = BeautifulSoup(str(html), \"html.parser\")\n",
    "    return len(s.find_all('a')), len(s.find_all('meta'))\n",
    "df_raw[['link_count','meta_count']] = df_raw['html_content'].apply(lambda h: pd.Series(tag_counts(h)))\n",
    "df_raw['flesch'] = df_raw['clean_text'].apply(lambda t: textstat.flesch_reading_ease(t) if len(t.split())>10 else np.nan)\n",
    "\n",
    "df = df_raw.copy()\n",
    "df.to_csv(\"data/features_intermediate.csv\", index=False)\n",
    "print(\"Saved features_intermediate.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a9cec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- TF-IDF + SVD ----------\n",
    "vectorizer = TfidfVectorizer(max_features=2000, stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(df['clean_text'].fillna(\"\"))\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "tfidf_svd = svd.fit_transform(tfidf)   # dense nx50\n",
    "\n",
    "# append svd features\n",
    "svd_cols = [f\"svd_{i}\" for i in range(tfidf_svd.shape[1])]\n",
    "df_svd = pd.DataFrame(tfidf_svd, columns=svd_cols, index=df.index)\n",
    "df = pd.concat([df, df_svd], axis=1)\n",
    "\n",
    "# Save tfidf matrix for duplicates search (optional)\n",
    "from scipy.sparse import save_npz\n",
    "save_npz(\"data/tfidf_matrix.npz\", tfidf)\n",
    "\n",
    "# persist vectorizer + svd\n",
    "joblib.dump(vectorizer, \"models/tfidf_vectorizer.pkl\")\n",
    "joblib.dump(svd, \"models/tfidf_svd.pkl\")\n",
    "print(\"Saved TF-IDF vectorizer and SVD\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d934cac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- LABELING per assignment rules (Low/Medium/High) ----------\n",
    "def assign_label(row):\n",
    "    wc = row['word_count']\n",
    "    flesch = row['flesch'] if not np.isnan(row['flesch']) else 50.0\n",
    "    if (wc > 1500) and (50 <= flesch <= 70):\n",
    "        return \"High\"\n",
    "    if (wc < 500) or (flesch < 30):\n",
    "        return \"Low\"\n",
    "    return \"Medium\"\n",
    "\n",
    "df['quality_label'] = df.apply(assign_label, axis=1)\n",
    "print(\"Label counts before relaxation:\\n\", df['quality_label'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546cc0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If any class missing or extremely underrepresented, relax thresholds:\n",
    "if df['quality_label'].nunique() < 2 or df['quality_label'].value_counts().min() < 3:\n",
    "    def assign_relaxed(r):\n",
    "        wc = r['word_count']\n",
    "        if wc < 800: return \"Low\"\n",
    "        if wc > 1200: return \"High\"\n",
    "        return \"Medium\"\n",
    "    df['quality_label'] = df.apply(assign_relaxed, axis=1)\n",
    "    print(\"Label counts after relaxation:\\n\", df['quality_label'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab23cab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- FEATURES and TARGET ----------\n",
    "num_cols = ['word_count','link_count','meta_count','flesch'] + svd_cols\n",
    "X = df[num_cols].fillna(0).values\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['quality_label'].values)\n",
    "print(\"Classes mapping:\", dict(zip(le.classes_, range(len(le.classes_)))))\n",
    "\n",
    "# Optional balancing: if a class has very few examples, we will undersample majority classes\n",
    "from collections import Counter\n",
    "cnt = Counter(y)\n",
    "print(\"Class distribution:\", cnt)\n",
    "# If needed, create a small balanced sample for training:\n",
    "if min(cnt.values()) < 5:\n",
    "    # simple undersample to ensure at least 5 per class (keeps deterministic)\n",
    "    df_bal = pd.concat([\n",
    "        df[df['quality_label'] == cls].sample(n=min(max(5, min(cnt.values())), len(df[df['quality_label'] == cls])), random_state=42)\n",
    "        for cls in df['quality_label'].unique()\n",
    "    ])\n",
    "    X = df_bal[num_cols].fillna(0).values\n",
    "    y = le.transform(df_bal['quality_label'].values)\n",
    "    print(\"After undersample distribution:\", Counter(y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872b4b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- TRAIN / EVALUATE ----------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)\n",
    "clf = RandomForestClassifier(n_estimators=300, random_state=42, class_weight='balanced_subsample')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "# cross-val\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "try:\n",
    "    cv_scores = cross_val_score(clf, X, y, cv=cv, scoring='accuracy')\n",
    "    print(\"CV accuracy (5-fold):\", cv_scores.mean(), cv_scores)\n",
    "except Exception as e:\n",
    "    print(\"CV skipped:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e435863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Save artifacts ----------\n",
    "joblib.dump(clf, \"models/quality_model.pkl\")\n",
    "joblib.dump(le, \"models/label_encoder.pkl\")\n",
    "joblib.dump(num_cols, \"models/feature_names.pkl\")\n",
    "print(\"Saved classifier and encoders to models/\")\n",
    "\n",
    "# Print top feature importances for inspection\n",
    "importances = clf.feature_importances_\n",
    "top_idx = np.argsort(importances)[-20:][::-1]\n",
    "top_features = [(num_cols[i], importances[i]) for i in top_idx if i < len(num_cols)]\n",
    "print(\"Top features:\", top_features[:15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c872a658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL EVALUATION & VISUALIZATION\n",
    "# ===========================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, ConfusionMatrixDisplay,\n",
    "    classification_report, accuracy_score, f1_score\n",
    ")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\n",
    "plt.figure(figsize=(6,5))\n",
    "disp.plot(values_format=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - SEO Quality Classifier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c39d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy & F1 Score\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "test_f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "print(\"\\n Final Model Performance\")\n",
    "print(\"Test Accuracy:\", round(test_acc, 3))\n",
    "print(\"Test F1 Score:\", round(test_f1, 3))\n",
    "\n",
    "print(\"\\n Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2793166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Bar Plot\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[-15:][::-1]  # top 15\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=importances[indices], y=np.array(num_cols)[indices])\n",
    "plt.title(\"Top 15 Important Features\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Feature Name\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8f8807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC CURVES (multiclass)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "y_test_bin = label_binarize(y_test, classes=list(range(len(le.classes_))))\n",
    "y_score = clf.predict_proba(X_test)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for i, class_name in enumerate(le.classes_):\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f\"{class_name} (AUC={roc_auc:.2f})\")\n",
    "\n",
    "plt.plot([0,1],[0,1],'--')\n",
    "plt.title(\"ROC Curves - Multi-Class\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cd269f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (seo_app)",
   "language": "python",
   "name": "seo_app"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
